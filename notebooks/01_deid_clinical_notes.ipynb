{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sppandlkk/healthcare-nlp-llm-pipelines/blob/main/notebooks/01_deid_clinical_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# De-identification of Clinical Notes using NLP\n",
        "\n",
        "This project focuses on de-identifying clinical notes to protect patient privacy while maintaining the clinical utility of the text. Approach and key steps:\n",
        "\n",
        "- Explored multiple NLP approaches: Implemented rule-based methods and pretrained NER models such as Microsoft Presidio and BERT-based models.\n",
        "- Leveraged LLMs for improved flexibility:\n",
        "  - Designed zero-shot prompts and refined them through prompt engineering for better name extraction accuracy.\n",
        "  - Addressed challenges with tokenization misalignment by manually recalculating character indices for evaluation.\n",
        "- Performed systematic evaluation: Compared traditional NLP models with LLM-based approaches to analyze trade-offs in accuracy, generalization, and human oversight requirements."
      ],
      "metadata": {
        "id": "Oxdm8T6F0UH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lSrlLmxHVIcy"
      },
      "outputs": [],
      "source": [
        "# python pacakge and import\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install presidio-analyzer presidio-anonymizer\n",
        "!pip install -q -U google-generativeai\n",
        "\n",
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForTokenClassification, AutoModelForCausalLM\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Synthetic Notes\n",
        "\n",
        "I generate synthetic clinical notes that contain multiple PHI entities, such as patient names, provider names, and family members. Each entity is manually annotated with its start and end character indices to serve as ground truth for evaluation.\n",
        "\n",
        "Note: The synthetic note includes intentional errors such as missing spaces in names (NurseKate) to test the robustness of de-identification models."
      ],
      "metadata": {
        "id": "YhwK0fw_wsde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create synthetic notes. Notice that I have NurseKate (missing space) to tell whether model can flag it\n",
        "note_text = \"\"\"\n",
        "Patient Emma M. Su underwent inpatient surgery for acute exacerbation of asthma and was admitted to Happy Valley Hospital for further management. During her stay, she received treatment with IV steroids, bronchodilators, and oxygen therapy. She was also seen by Dr. Lee, a pulmonologist affiliated with Pulmonary Department, who adjusted her medication regimen. The patient's family members, including brother Adam, Liv, and Dave  Ledger (partner), visited her regularly and provided emotional support. The dad, PeteSu, expressed concern about her condition and stated \"I'm glad she's getting the care she needs\". Her mom, Jen K, will be picking her up from Happy Valley Hospital's Discharge Unit today after discharge.\n",
        "During her stay, the patient underwent various tests, including pulmonary function tests and chest X-rays, which showed significant improvement after treatment. These tests were conducted by Support Pulmonary Function Laboratory. The patient was also educated on proper inhaler use and asthma management by NurseKate.\n",
        "She will follow up with Dr. Smith in 2 weeks to reassess her symptoms and adjust her medication regimen as needed. Her friend, mike hope, will be helping her with errands and chores during her recovery.\n",
        "The patient's condition improved significantly during her stay, and she was discharged in stable condition with instructions to rest and continue her medication regimen. Emma's condition will continue to be monitored by her healthcare team, including Dr. Smith Y. and Nurse Kate W. from Happy Valley Hospital.\n",
        "Documented by: Kate Whittier. Signed by: Dr. Smith M Yeats, MD. Date: March 15, 2023, 14:30\n",
        "\"\"\"\n",
        "# remove leading space, \\n, tab\n",
        "note_text = note_text.lstrip()\n",
        "\n",
        "# manually annotate entity_text\n",
        "ground_truth = pd.DataFrame(\n",
        "        {\n",
        "        \"entity_text\":[\"Emma M. Su\", \"Lee\", \"Adam\", \"Liv\", \"Dave  Ledger\", \"PeteSu\", \"Jen K\", \"Kate\", \"Smith\", \"mike hope\", \"Smith Y.\", \"Kate W.\", \"Kate Whittier\", \"Smith M Yeats\"],\n",
        "        \"entity_start_index\":[8, 266, 410, 416, 425, 512, 623, 1032, 1066, 1165, 1496, 1515, 1566, 1596],\n",
        "        \"entity_end_index\":[18, 269, 414, 419, 437, 518, 628, 1036, 1071, 1174, 1504, 1522, 1579, 1609]\n",
        "    }\n",
        ")\n",
        "ground_truth"
      ],
      "metadata": {
        "id": "XQhbKMbgVPCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Microsoft Presidio Model\n",
        "\n",
        "[Microsoft Presidio](https://github.com/microsoft/presidio) is an open-source library for detecting personally identifiable information (PII) in text. In this section, we apply the Presidio recognizer to our synthetic note to automatically detect names. Later, we will compare the predicted entities with our ground truth annotations.\n",
        "\n",
        "Presidio outputs start/end indices and entity types, which can be directly compared to ground truth for evaluation."
      ],
      "metadata": {
        "id": "1IstNltexOBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize presidio_analyzer\n",
        "analyzer = AnalyzerEngine()\n",
        "# detect PII\n",
        "results = analyzer.analyze(text=note_text, entities=[\"PERSON\"], language=\"en\")\n",
        "df_presidio = pd.DataFrame([\n",
        "    {\n",
        "        \"model\":\"presidio\",\n",
        "        \"entity_type\":ent.entity_type,\n",
        "        \"entity_start_index\":ent.start,\n",
        "        \"entity_end_index\":ent.end,\n",
        "        \"entity_text\":note_text[ent.start:ent.end]\n",
        "    } for ent in results\n",
        "])\n",
        "df_presidio"
      ],
      "metadata": {
        "id": "V7PV0NfRWxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative NER Models from HuggingFace\n",
        "- BERT-Base NER ([dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)):\n",
        "A smaller BERT model fine-tuned on the CoNLL-2003 dataset to recognize person names and other standard entities.\n",
        "\n",
        "- BERT-Large NER ([dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)):\n",
        "A larger cased BERT model fine-tuned similarly on the CoNLL-2003 dataset, expected to provide better context understanding due to more parameters.\n",
        "\n",
        "We use the HuggingFace pipeline API for NER and extract entities including start/end positions to compare with ground truth."
      ],
      "metadata": {
        "id": "fveTVtsFyACX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a small NER model for demo\n",
        "bert_base = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "ents = bert_base(note_text)\n",
        "df_bert_base = pd.DataFrame([\n",
        "    {\n",
        "        \"model\":\"bert_base\",\n",
        "        \"entity_type\": ent[\"entity_group\"],\n",
        "        \"entity_start_index\": ent[\"start\"],\n",
        "        \"entity_end_index\": ent[\"end\"],\n",
        "        \"entity_text\": ent[\"word\"]\n",
        "    } for ent in ents if ent[\"entity_group\"] == \"PER\"\n",
        "])\n",
        "df_bert_base"
      ],
      "metadata": {
        "id": "JY6E2huhWrfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Build NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "# Run NER\n",
        "ents = ner_pipeline(note_text)\n",
        "df_bert_large = pd.DataFrame([\n",
        "    {\n",
        "        \"model\": \"bert_large\",\n",
        "        \"entity_type\": ent[\"entity_group\"],\n",
        "        \"entity_start_index\": ent[\"start\"],\n",
        "        \"entity_end_index\": ent[\"end\"],\n",
        "        \"entity_text\": ent[\"word\"]\n",
        "    } for ent in ents if ent[\"entity_group\"] == \"PER\"\n",
        "])\n",
        "df_bert_large"
      ],
      "metadata": {
        "id": "Qxk0Tw56gpWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "Model performance is evaluated using overlap ratio thresholds. If a predicted span overlaps a ground truth span above a threshold, it is counted as a true positive; otherwise, it is a false negative.\n",
        "\n",
        "Metrics computed include:\n",
        "\n",
        "Precision: TP / (TP + FP) – indicates how much correct information is retained without over-censoring.\n",
        "\n",
        "Recall: TP / (TP + FN) – more critical for de-identification, since missing a PHI entity could result in sensitive information leakage.\n",
        "\n",
        "Multiple thresholds are applied to assess robustness, and Seaborn line plots are used to visualize precision and recall across thresholds for each model."
      ],
      "metadata": {
        "id": "5cdT8YURznXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ner_models(ground_truth, model_dfs, model_names, thresholds=[0.1, 0.2, 0.3, 0.4, 0.5]):\n",
        "    \"\"\"\n",
        "    Evaluate multiple NER models against ground truth using overlap-based matching.\n",
        "\n",
        "    TP is counted when overlap / ground truth span >= threshold.\n",
        "    FP is counted when overlap / predicted span < threshold.\n",
        "    FN is counted for ground truth entities that were not matched.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth : pd.DataFrame\n",
        "        DataFrame containing ground truth entities with columns:\n",
        "        [\"entity_text\", \"entity_start_index\", \"entity_end_index\"].\n",
        "    model_dfs : list of pd.DataFrame\n",
        "        List of predicted entities DataFrames, each with the same columns as ground_truth.\n",
        "    model_names : list of str\n",
        "        Names of the models corresponding to model_dfs.\n",
        "    thresholds : list of float, optional\n",
        "        Minimum overlap ratio to consider a predicted entity as a true positive.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing precision and recall for each model at each threshold.\n",
        "        Columns: [\"model\", \"threshold\", \"precision\", \"recall\"].\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for model_df, model_name in zip(model_dfs, model_names):\n",
        "        for t in thresholds:\n",
        "            tp, fp, fn = 0, 0, 0\n",
        "            matched_gt_idx = set()\n",
        "\n",
        "            for _, m_row in model_df.iterrows():\n",
        "                m_start, m_end = m_row[\"entity_start_index\"], m_row[\"entity_end_index\"]\n",
        "\n",
        "                best_overlap_gt = 0\n",
        "                best_overlap_pred = 0\n",
        "                best_gt_idx = None\n",
        "\n",
        "                for gt_idx, gt_row in ground_truth.iterrows():\n",
        "                    gt_start, gt_end = gt_row[\"entity_start_index\"], gt_row[\"entity_end_index\"]\n",
        "\n",
        "                    overlap = max(0, min(m_end, gt_end) - max(m_start, gt_start))\n",
        "                    if overlap > 0:\n",
        "                        overlap_gt = overlap / (gt_end - gt_start)\n",
        "                        overlap_pred = overlap / (m_end - m_start)\n",
        "\n",
        "                        if overlap_gt > best_overlap_gt:\n",
        "                            best_overlap_gt = overlap_gt\n",
        "                            best_gt_idx = gt_idx\n",
        "                        if overlap_pred > best_overlap_pred:\n",
        "                            best_overlap_pred = overlap_pred\n",
        "\n",
        "                # Count TP if ground truth overlap >= threshold\n",
        "                if best_overlap_gt >= t:\n",
        "                    tp += 1\n",
        "                    matched_gt_idx.add(best_gt_idx)\n",
        "\n",
        "                # Count FP if predicted entity overlap < threshold\n",
        "                if best_overlap_pred < t:\n",
        "                    fp += 1\n",
        "\n",
        "            fn = len(ground_truth) - len(matched_gt_idx)\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "            results.append({\n",
        "                \"model\": model_name,\n",
        "                \"threshold\": t,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "N7T0YwOddFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = \\\n",
        "evaluate_ner_models(ground_truth,\n",
        "                    model_dfs=[df_presidio, df_bert_base, df_bert_large],\n",
        "                    model_names=[\"Presidio\", \"BERT Base\", \"BERT Large\"],\n",
        "                    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "evaluation_result"
      ],
      "metadata": {
        "id": "rID3em3dnCyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(df, metric=\"precision\"):\n",
        "    \"\"\"\n",
        "    Plot evaluation results for precision or recall across thresholds.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with columns [\"model\", \"threshold\", \"precision\", \"recall\"].\n",
        "        metric (str): One of [\"precision\", \"recall\"].\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.lineplot(\n",
        "        data=df,\n",
        "        x=\"threshold\",\n",
        "        y=metric,\n",
        "        hue=\"model\",\n",
        "        style=\"model\",    # <-- this makes each model different line/marker\n",
        "        markers=True,     # <-- enable markers\n",
        "        dashes=True,      # <-- enable dashed lines\n",
        "        palette=\"tab10\",  # <-- better color separation\n",
        "        alpha=0.9\n",
        "    )\n",
        "    plt.title(f\"{metric.capitalize()} across thresholds\", fontsize=14)\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.xlabel(\"Overlap threshold\")\n",
        "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zmXfgzjEniyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"recall\")"
      ],
      "metadata": {
        "id": "UBAPUI_ipOIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"precision\")"
      ],
      "metadata": {
        "id": "j3smnjFbrq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs with Zero-Shot and Refined Prompts\n",
        "\n",
        "Beyond traditional NER models, I experimented with large language models (LLMs) through API calls to perform de-identification. Initially, I designed a zero-shot prompt instructing the LLM (Gemini 1.5 Flash) to extract all person names from clinical notes and return them in a structured JSON format.\n",
        "\n",
        "To improve reliability and consistency of the outputs, I applied prompt engineering techniques, including:\n",
        "\n",
        "- Constraint-Based Prompting: Specifying strict output requirements (e.g., removing prefixes such as Dr. or Nurse, preserving double spaces).\n",
        "\n",
        "- Few-Shot Prompting: Providing illustrative examples to guide the model toward the desired format.\n",
        "\n",
        "- Chain-of-Thought Guidance: Encouraging step-by-step reasoning for more accurate name detection.\n",
        "\n",
        "These refinements significantly improved name extraction quality without retraining the model, demonstrating the flexibility of LLMs for domain-specific tasks. However, LLMs consistently struggled with returning correct indices due to tokenization differences, even after prompt improvements. Therefore, manual recalculation of indices was required for accurate evaluation."
      ],
      "metadata": {
        "id": "cPO76cOblsTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use your own key. Here I am using Colab secret to store my key\n",
        "genai.configure(api_key=userdata.get(\"gemini_key\"))\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
        "\n",
        "# provide prompt\n",
        "prompt1 = f\"\"\"\n",
        "Extract all PERSON names from the following clinical note.\n",
        "Return ONLY a JSON array with (start index, end index, and name)\n",
        "\n",
        "Clinical Note:\n",
        "\n",
        "{note_text}\n",
        "\"\"\"\n",
        "\n",
        "response1 = model.generate_content(prompt1)\n",
        "print(response1.text)\n"
      ],
      "metadata": {
        "id": "w3TqVKZcmMr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = f\"\"\"\n",
        "You are an expert in text annotation and entity extraction.\n",
        "\n",
        "Your task:\n",
        "1. Extract all PERSON names from the following clinical note exactly as they appear in the text.\n",
        "2. For each name, return its character-level start and end indices from the ORIGINAL text (0-based, inclusive start, exclusive end).\n",
        "3. If a name has a prefix (e.g., \"Dr.\" or \"Nurse\"), remove ONLY the prefix from the name and adjust indices accordingly.\n",
        "4. Do NOT normalize spaces: if the name has double spaces, keep them in both the extracted text and when calculating indices.\n",
        "5. Validate that the substring using these indices exactly matches the extracted name. If it does not match, fix the indices.\n",
        "\n",
        "Follow these steps:\n",
        "- Step 1: Identify all names.\n",
        "- Step 2: Calculate start and end indices.\n",
        "- Step 3: For each, double-check by substring validation.\n",
        "- Step 4: Return only the final JSON array in the format:\n",
        "[\n",
        "  {{\"start_index\": int, \"end_index\": int, \"name\": \"string\"}},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Example:\n",
        "Original Text: \"Dr. Alex met Nurse Bob.\"\n",
        "Output:\n",
        "[\n",
        "  {{\"start_index\": 4, \"end_index\": 8, \"name\": \"Alex\"}},\n",
        "  {{\"start_index\": 18, \"end_index\": 21, \"name\": \"Bob\"}}\n",
        "]\n",
        "\n",
        "Now process the following clinical note:\n",
        "\n",
        "{note_text}\n",
        "\n",
        "JSON:\n",
        "\"\"\"\n",
        "response2 = model.generate_content(prompt2)\n",
        "print(response2.text)\n"
      ],
      "metadata": {
        "id": "YYG0Q9O4wkSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Index Alignment Issues\n",
        "\n",
        "While prompt engineering improved the accuracy of entity extraction, I encountered a major limitation: the indices returned by the LLM were often wrong.\n",
        "\n",
        "This happens because LLMs process input through tokenization, which breaks alignment with the raw character positions in the original text.\n",
        "\n",
        "\n",
        "To address this, I manually recalculated the indices for the extracted entities. With correct indices, I was able to run a proper evaluation against other models.\n"
      ],
      "metadata": {
        "id": "-jGtmkFp9CMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract json and load\n",
        "match = re.search(r\"\"\"\\[.*\\]\"\"\", response2.text, re.DOTALL)\n",
        "json_result = json.loads(match.group(0))\n",
        "names = [item[\"name\"] for item in json_result]\n",
        "names"
      ],
      "metadata": {
        "id": "DhPkYK0qrqyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_entities_with_indices(note_text, names):\n",
        "    results = []\n",
        "    search_start = 0  # where to start searching in the note\n",
        "\n",
        "    for name in names:\n",
        "        idx = note_text.find(name, search_start)\n",
        "        if idx == -1:\n",
        "            # if not found, skip or raise warning\n",
        "            print(f\"Warning: '{name}' not found in text after position {search_start}\")\n",
        "            continue\n",
        "\n",
        "        start_idx = idx\n",
        "        end_idx = idx + len(name)\n",
        "        results.append({\n",
        "            \"model\":\"gemini-1.5-flash\",\n",
        "            \"entity_type\":\"PERSON\",\n",
        "            \"entity_start_index\":start_idx,\n",
        "            \"entity_end_index\":end_idx,\n",
        "            \"entity_text\":name\n",
        "        })\n",
        "\n",
        "        # move the search start forward to avoid re-finding the same text\n",
        "        search_start = end_idx\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df_gemini = align_entities_with_indices(note_text, names)\n",
        "df_gemini"
      ],
      "metadata": {
        "id": "EgqXxz90isUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = \\\n",
        "evaluate_ner_models(ground_truth,\n",
        "                    model_dfs=[df_presidio, df_bert_base, df_bert_large, df_gemini],\n",
        "                    model_names=[\"Presidio\", \"BERT Base\", \"BERT Large\", \"Gemini 1.5 Flash\"],\n",
        "                    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "evaluation_result"
      ],
      "metadata": {
        "id": "fMahXNZpIOmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"recall\")"
      ],
      "metadata": {
        "id": "WDIuFWD52OXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"precision\")"
      ],
      "metadata": {
        "id": "UNPGQQXD5P-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: NLP vs. LLM For De-identifying Names\n",
        "| Feature                         | NLP Models (NER)                                           | LLMs                                                                |\n",
        "| ------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| **Training process**            | Task-specific, trained for de-identification               | General-purpose, requires prompt engineering                        |\n",
        "| **Ease of use & adaptability**  | Easy to use within training domain, limited outside domain | Generalizes to unseen contexts, can improve with prompt engineering |\n",
        "| **Index extraction capability** | Accurate character positions                               | Tokenization can break alignment, manual correction may be needed   |\n",
        "| **Human oversight**             | Minimal                                                    | Important to ensure correctness                                     |\n",
        "\n",
        "\n",
        "**Summary**:\n",
        "Traditional NLP models perform robustly for structured, task-specific de-identification tasks, delivering consistent and precise entity extraction. LLMs, in contrast, can handle unstructured or complex clinical notes, identifying a broader range of entities, but require careful human oversight to address tokenization and index alignment issues.\n",
        "\n",
        "**Note**: Because this project uses synthetic data, the exact ranking or performance differences between models may not fully reflect real-world behavior, but it effectively demonstrates the methodology and evaluation approach."
      ],
      "metadata": {
        "id": "zJ2Vut0z9g7J"
      }
    }
  ]
}