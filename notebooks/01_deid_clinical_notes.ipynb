{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMC5n/G68jhFfM1nylbd31J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sppandlkk/healthcare-nlp-llm-pipelines/blob/main/notebooks/01_deid_clinical_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# De-identification of Clinical Notes using NLP\n",
        "This project focuses on de-identification of clinical notes with the goal of protecting patient privacy while preserving the medical utility of the text. My main contributions are:\n",
        "\n",
        "- Experimented with rule-based and pretrained NER models, such as Microsoft Presidio and BERT-based models.\n",
        "- Integrated LLM-based approaches to test whether large language models can improve recall in entity detection.\n",
        "  - Applied prompt engineering to improve zero-shot LLM performance.\n",
        "  - Manually corrected issues produced by LLM outputs that lost alignment due to tokenization.\n",
        "- Conducted evaluation and comparison between NER models and LLM-based approaches."
      ],
      "metadata": {
        "id": "Oxdm8T6F0UH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lSrlLmxHVIcy"
      },
      "outputs": [],
      "source": [
        "# python pacakge and import\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install presidio-analyzer presidio-anonymizer\n",
        "!pip install -q -U google-generativeai\n",
        "\n",
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForTokenClassification, AutoModelForCausalLM\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Synthetic Notes\n",
        "\n",
        "I generate synthetic clinical notes that contain multiple PHI entities, such as patient names, provider names, and family members. Each entity is manually annotated with its start and end character indices to serve as ground truth for evaluation.\n",
        "\n",
        "Note: The synthetic note includes intentional errors such as missing spaces in names (NurseKate) to test the robustness of de-identification models."
      ],
      "metadata": {
        "id": "YhwK0fw_wsde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create synthetic notes. Notice that I have NurseKate (missing space) to tell whether model can flag it\n",
        "note_text = \"\"\"\n",
        "Patient Emma M. Su underwent inpatient surgery for acute exacerbation of asthma and was admitted to Happy Valley Hospital for further management. During her stay, she received treatment with IV steroids, bronchodilators, and oxygen therapy. She was also seen by Dr. Lee, a pulmonologist affiliated with Pulmonary Department, who adjusted her medication regimen. The patient's family members, including brother Adam, Liv, and Dave  Ledger (partner), visited her regularly and provided emotional support. The dad, PeteSu, expressed concern about her condition and stated \"I'm glad she's getting the care she needs\". Her mom, Jen K, will be picking her up from Happy Valley Hospital's Discharge Unit today after discharge.\n",
        "During her stay, the patient underwent various tests, including pulmonary function tests and chest X-rays, which showed significant improvement after treatment. These tests were conducted by Support Pulmonary Function Laboratory. The patient was also educated on proper inhaler use and asthma management by NurseKate.\n",
        "She will follow up with Dr. Smith in 2 weeks to reassess her symptoms and adjust her medication regimen as needed. Her friend, mike hope, will be helping her with errands and chores during her recovery.\n",
        "The patient's condition improved significantly during her stay, and she was discharged in stable condition with instructions to rest and continue her medication regimen. Emma's condition will continue to be monitored by her healthcare team, including Dr. Smith Y. and Nurse Kate W. from Happy Valley Hospital.\n",
        "Documented by: Kate Whittier. Signed by: Dr. Smith M Yeats, MD. Date: March 15, 2023, 14:30\n",
        "\"\"\"\n",
        "\n",
        "# manually annotate entity_text\n",
        "ground_truth = pd.DataFrame(\n",
        "        {\n",
        "        \"entity_text\": [\"Emma M. Su\", \"Lee\", \"Adam\", \"Liv\", \"Dave  Ledger\", \"PeteSu\", \"Jen K\", \"Kate\", \"Smith\", \"mike hope\", \"Smith Y.\", \"Kate W.\", \"Kate Whittier\", \"Smith M Yeats\"],\n",
        "        \"entity_start_index\": [9, 267, 411, 417, 426, 513, 624, 1033, 1067, 1166, 1497, 1516, 1567, 1597],\n",
        "        \"entity_end_index\": [19, 270, 415, 420, 438, 519, 629, 1037, 1072, 1175, 1505, 1523, 1580, 1610]\n",
        "    }\n",
        ")\n",
        "ground_truth"
      ],
      "metadata": {
        "id": "XQhbKMbgVPCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Microsoft Presidio Model\n",
        "\n",
        "[Microsoft Presidio](https://github.com/microsoft/presidio) is an open-source library for detecting personally identifiable information (PII) in text. In this section, we apply the Presidio recognizer to our synthetic note to automatically detect names. Later, we will compare the predicted entities with our ground truth annotations.\n",
        "\n",
        "Presidio outputs start/end indices and entity types, which can be directly compared to ground truth for evaluation."
      ],
      "metadata": {
        "id": "1IstNltexOBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize presidio_analyzer\n",
        "analyzer = AnalyzerEngine()\n",
        "# detect PII\n",
        "results = analyzer.analyze(text=note_text, entities=[\"PERSON\"], language=\"en\")\n",
        "df_presidio = pd.DataFrame([\n",
        "    {\n",
        "        \"model\":\"presidio\",\n",
        "        \"entity_type\":ent.entity_type,\n",
        "        \"entity_start_index\":ent.start,\n",
        "        \"entity_end_index\":ent.end,\n",
        "        \"entity_text\":note_text[ent.start:ent.end]\n",
        "    } for ent in results\n",
        "])\n",
        "df_presidio"
      ],
      "metadata": {
        "id": "V7PV0NfRWxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative NER Models from HuggingFace\n",
        "- BERT-Base NER ([dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)):\n",
        "A smaller BERT model fine-tuned on the CoNLL-2003 dataset to recognize person names and other standard entities.\n",
        "\n",
        "- BERT-Large NER ([dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)):\n",
        "A larger cased BERT model fine-tuned similarly on the CoNLL-2003 dataset, expected to provide better context understanding due to more parameters.\n",
        "\n",
        "We use the HuggingFace pipeline API for NER and extract entities including start/end positions to compare with ground truth."
      ],
      "metadata": {
        "id": "fveTVtsFyACX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a small NER model for demo\n",
        "bert_base = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "ents = bert_base(note_text)\n",
        "df_bert_base = pd.DataFrame([\n",
        "    {\n",
        "        \"model\":\"bert_base\",\n",
        "        \"entity_type\": ent[\"entity_group\"],\n",
        "        \"entity_start_index\": ent[\"start\"],\n",
        "        \"entity_end_index\": ent[\"end\"],\n",
        "        \"entity_text\": ent[\"word\"]\n",
        "    } for ent in ents if ent[\"entity_group\"] == \"PER\"\n",
        "])\n",
        "df_bert_base"
      ],
      "metadata": {
        "id": "JY6E2huhWrfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Build NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "# Run NER\n",
        "ents = ner_pipeline(note_text)\n",
        "df_bert_large = pd.DataFrame([\n",
        "    {\n",
        "        \"model\": \"bert_large\",\n",
        "        \"entity_type\": ent[\"entity_group\"],\n",
        "        \"entity_start_index\": ent[\"start\"],\n",
        "        \"entity_end_index\": ent[\"end\"],\n",
        "        \"entity_text\": ent[\"word\"]\n",
        "    } for ent in ents if ent[\"entity_group\"] == \"PER\"\n",
        "])\n",
        "df_bert_large"
      ],
      "metadata": {
        "id": "Qxk0Tw56gpWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "Model performance is evaluated using overlap ratio thresholds. If a predicted span overlaps a ground truth span above a threshold, it is counted as a true positive; otherwise, it is a false negative.\n",
        "\n",
        "Metrics computed include:\n",
        "\n",
        "Precision: TP / (TP + FP) – indicates how much correct information is retained without over-censoring.\n",
        "\n",
        "Recall: TP / (TP + FN) – more critical for de-identification, since missing a PHI entity could result in sensitive information leakage.\n",
        "\n",
        "Multiple thresholds are applied to assess robustness, and Seaborn line plots are used to visualize precision and recall across thresholds for each model."
      ],
      "metadata": {
        "id": "5cdT8YURznXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ner_models(ground_truth, model_dfs, model_names, thresholds=[0.1, 0.2, 0.3, 0.4, 0.5]):\n",
        "    \"\"\"\n",
        "    Evaluate multiple NER models against ground truth using overlap-based matching.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth : pd.DataFrame\n",
        "        DataFrame containing ground truth entities with columns:\n",
        "        [\"entity_text\", \"entity_start_index\", \"entity_end_index\"].\n",
        "    model_dfs : list of pd.DataFrame\n",
        "        List of predicted entities DataFrames, each with the same columns as ground_truth.\n",
        "    model_names : list of str\n",
        "        Names of the models corresponding to model_dfs.\n",
        "    thresholds : list of float, optional\n",
        "        Minimum overlap ratio to consider a predicted entity as a true positive.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing precision and recall for each model at each threshold.\n",
        "        Columns: [\"model\", \"threshold\", \"precision\", \"recall\"].\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for model_df, model_name in zip(model_dfs, model_names):\n",
        "        for t in thresholds:\n",
        "            tp, fp, fn = 0, 0, 0\n",
        "            matched_gt_idx = set()\n",
        "\n",
        "            # Iterate over predicted entities\n",
        "            for _, m_row in model_df.iterrows():\n",
        "                m_start, m_end = m_row[\"entity_start_index\"], m_row[\"entity_end_index\"]\n",
        "                match_found = False\n",
        "\n",
        "                for gt_idx, gt_row in ground_truth.iterrows():\n",
        "                    gt_start, gt_end = gt_row[\"entity_start_index\"], gt_row[\"entity_end_index\"]\n",
        "\n",
        "                    # Compute overlap\n",
        "                    overlap = max(0, min(m_end, gt_end) - max(m_start, gt_start))\n",
        "                    overlap_ratio = overlap / (gt_end - gt_start)\n",
        "\n",
        "                    if overlap_ratio >= t:\n",
        "                        tp += 1\n",
        "                        matched_gt_idx.add(gt_idx)\n",
        "                        match_found = True\n",
        "                        break\n",
        "\n",
        "                if not match_found:\n",
        "                    fp += 1\n",
        "\n",
        "            fn = len(ground_truth) - len(matched_gt_idx)\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "            results.append({\n",
        "                \"model\": model_name,\n",
        "                \"threshold\": t,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "N7T0YwOddFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = \\\n",
        "evaluate_ner_models(ground_truth,\n",
        "                    model_dfs=[df_presidio, df_bert_base, df_bert_large],\n",
        "                    model_names=[\"Presidio\", \"BERT Base\", \"BERT Large\"],\n",
        "                    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "evaluation_result"
      ],
      "metadata": {
        "id": "rID3em3dnCyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(df, metric=\"precision\"):\n",
        "    \"\"\"\n",
        "    Plot evaluation results for precision or recall across thresholds.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with columns [\"model\", \"threshold\", \"precision\", \"recall\"].\n",
        "        metric (str): One of [\"precision\", \"recall\"].\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.lineplot(\n",
        "        data=df,\n",
        "        x=\"threshold\",\n",
        "        y=metric,\n",
        "        hue=\"model\",\n",
        "        style=\"model\",    # <-- this makes each model different line/marker\n",
        "        markers=True,     # <-- enable markers\n",
        "        dashes=True,      # <-- enable dashed lines\n",
        "        palette=\"tab10\",  # <-- better color separation\n",
        "        alpha=0.9\n",
        "    )\n",
        "    plt.title(f\"{metric.capitalize()} across thresholds\", fontsize=14)\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.xlabel(\"Overlap threshold\")\n",
        "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zmXfgzjEniyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"recall\")"
      ],
      "metadata": {
        "id": "UBAPUI_ipOIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"precision\")"
      ],
      "metadata": {
        "id": "j3smnjFbrq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs with Zero-shot Prompts\n",
        "\n",
        "Beyond traditional NER models, I also experimented with large language models (LLMs) through API calls.\n",
        "\n",
        "- Design a zero-shot prompt that instructs the LLM (Gemini 1.5 Flash) to extract all person names from clinical notes and return them in a structured JSON format.\n",
        "- prompt engineer to refine the instructions to make the model output more reliable and consistently formatted.\n",
        "\n",
        "This step highlights how LLMs can adapt to tasks they were not explicitly trained on, using only carefully designed prompts."
      ],
      "metadata": {
        "id": "cPO76cOblsTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use your own key. Here I am using Colab secret to store my key\n",
        "genai.configure(api_key=userdata.get(\"gemini_key\"))\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
        "\n",
        "# provide prompt\n",
        "prompt1 = f\"\"\"\n",
        "Extract all PERSON names from the following clinical note.\n",
        "Return ONLY a JSON array with (start index, end index, and name)\n",
        "\n",
        "Clinical Note:\n",
        "\n",
        "{note_text}\n",
        "\"\"\"\n",
        "\n",
        "response1 = model.generate_content(prompt1)\n",
        "print(response1.text)\n"
      ],
      "metadata": {
        "id": "w3TqVKZcmMr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = f\"\"\"\n",
        "Extract all PERSON names from the following clinical note.\n",
        "Return ONLY a JSON array with (start index, end index, and name).\n",
        "If the name has prefix (Dr. or Nurse), remove the prefix and recalculate indices.\n",
        "If the name has double spaces, keep double spaces in the string.\n",
        "Do not replace double spaces by single space.\n",
        "\n",
        "Clinical Note:\n",
        "\n",
        "{note_text}\n",
        "\"\"\"\n",
        "response2 = model.generate_content(prompt2)\n",
        "print(response2.text)\n"
      ],
      "metadata": {
        "id": "ZTmsgFfvmVRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Index Alignment Issues\n",
        "\n",
        "While prompt engineering improved the accuracy of entity extraction, I encountered a major limitation: the indices returned by the LLM were often wrong.\n",
        "\n",
        "This happens because LLMs process input through tokenization, which breaks alignment with the raw character positions in the original text.\n",
        "\n",
        "\n",
        "To address this, I manually recalculated the indices for the extracted entities. With correct indices, I was able to run a proper evaluation against other models.\n"
      ],
      "metadata": {
        "id": "-jGtmkFp9CMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract json and load\n",
        "match = re.search(r\"\"\"\\[.*\\]\"\"\", response2.text, re.DOTALL)\n",
        "json_result = json.loads(match.group(0))\n",
        "names = [item[\"name\"] for item in json_result]\n",
        "names"
      ],
      "metadata": {
        "id": "DhPkYK0qrqyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_entities_with_indices(note_text, names):\n",
        "    results = []\n",
        "    search_start = 0  # where to start searching in the note\n",
        "\n",
        "    for name in names:\n",
        "        idx = note_text.find(name, search_start)\n",
        "        if idx == -1:\n",
        "            # if not found, skip or raise warning\n",
        "            print(f\"Warning: '{name}' not found in text after position {search_start}\")\n",
        "            continue\n",
        "\n",
        "        start_idx = idx\n",
        "        end_idx = idx + len(name)\n",
        "        results.append({\n",
        "            \"model\":\"gemini-1.5-flash\",\n",
        "            \"entity_type\":\"PERSON\",\n",
        "            \"entity_start_index\":start_idx,\n",
        "            \"entity_end_index\":end_idx,\n",
        "            \"entity_text\":name\n",
        "        })\n",
        "\n",
        "        # move the search start forward to avoid re-finding the same text\n",
        "        search_start = end_idx\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df_gemini = align_entities_with_indices(note_text, names)\n",
        "df_gemini"
      ],
      "metadata": {
        "id": "EgqXxz90isUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = \\\n",
        "evaluate_ner_models(ground_truth,\n",
        "                    model_dfs=[df_presidio, df_bert_base, df_bert_large, df_gemini],\n",
        "                    model_names=[\"Presidio\", \"BERT Base\", \"BERT Large\", \"Gemini 1.5 Flash\"],\n",
        "                    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "evaluation_result"
      ],
      "metadata": {
        "id": "yUN_S82eiz-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"recall\")"
      ],
      "metadata": {
        "id": "WDIuFWD52OXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(evaluation_result, \"precision\")"
      ],
      "metadata": {
        "id": "UNPGQQXD5P-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: NLP vs. LLM For De-identifying Names\n",
        "| Feature                         | NLP Models (NER)                                           | LLMs                                                                |\n",
        "| ------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| **Training process**            | Task-specific, trained for de-identification               | General-purpose, requires prompt engineering                        |\n",
        "| **Ease of use & adaptability**  | Easy to use within training domain, limited outside domain | Generalizes to unseen contexts, can improve with prompt engineering |\n",
        "| **Index extraction capability** | Accurate character positions                               | Tokenization can break alignment, manual correction may be needed   |\n",
        "| **Human oversight**             | Minimal                                                    | Important to ensure correctness                                     |\n",
        "\n",
        "\n",
        "**Summary**:\n",
        "Traditional NLP models perform robustly for structured, task-specific de-identification tasks, delivering consistent and precise entity extraction. LLMs, in contrast, can handle unstructured or complex clinical notes, identifying a broader range of entities, but require careful human oversight to address tokenization and index alignment issues.\n",
        "\n",
        "**Note**: Because this project uses synthetic data, the exact ranking or performance differences between models may not fully reflect real-world behavior, but it effectively demonstrates the methodology and evaluation approach."
      ],
      "metadata": {
        "id": "zJ2Vut0z9g7J"
      }
    }
  ]
}