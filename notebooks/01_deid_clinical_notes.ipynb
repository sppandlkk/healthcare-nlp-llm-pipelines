{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSG6pDigmlpY8Ch93EXhJm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sppandlkk/healthcare-nlp-llm-pipelines/blob/main/notebooks/01_deid_clinical_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# De-identification of Clinical Notes using NLP\n",
        "This project demonstrates de-identification of clinical notes using multiple natural language processing (NLP) techniques. We create synthetic clinical notes containing sensitive personal health information (PHI), manually annotate the entities, and evaluate the performance of various models,including Microsoft Presidio and HuggingFace NER models. The evaluation framework is designed to benchmark open-source models as well as proprietary/vendor solutions such as Tonic.ai and Private.ai. This framework leverages real-world EHR data to assess entity detection performance and robustness."
      ],
      "metadata": {
        "id": "Oxdm8T6F0UH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lSrlLmxHVIcy"
      },
      "outputs": [],
      "source": [
        "# python pacakge and import\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install presidio-analyzer presidio-anonymizer\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from presidio_analyzer import AnalyzerEngine#, RecognizerResult\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Synthetic Notes\n",
        "\n",
        "I generate synthetic clinical notes that contain multiple PHI entities, such as patient names, provider names, and family members. Each entity is manually annotated with its start and end character indices to serve as ground truth for evaluation.\n",
        "\n",
        "Note: The synthetic note includes intentional errors such as missing spaces in names (NurseKate) to test the robustness of de-identification models."
      ],
      "metadata": {
        "id": "YhwK0fw_wsde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create synthetic notes. Notice that I have NurseKate (missing space) to tell whether model can flag it\n",
        "note_text = \"\"\"\n",
        "Patient Emma Su underwent inpatient surgery for acute exacerbation of asthma and was admitted to the hospital for further management. During her stay, she received treatment with IV steroids, bronchodilators, and oxygen therapy. She was also seen by Dr. Lee, a pulmonologist, who adjusted her medication regimen. The patient's family members, including Rob (brother), Liv, and Dave (partner), visited her regularly and provided emotional support. The dad (Pete) expressed concern about her condition and stated \"I'm glad she's getting the care she needs, sweetie\". Her mom, Jen K, will be picking her up from the hospital today after discharge. During her stay, the patient underwent various tests, including pulmonary function tests and chest X-rays, which showed significant improvement after treatment. The patient was also educated on proper inhaler use and asthma management by NurseKate. She will follow up with Dr. Smith in 2 weeks to reassess her symptoms and adjust her medication regimen as needed. Her friend mike will be helping her with errands and chores during her recovery. The patient's condition improved significantly during her stay, and she was discharged in stable condition with instructions to rest and continue her medication regimen. Emma's condition will continue to be monitored by her healthcare team, including Dr. Smith Y. and Nurse Kate W. Documented by: Kate Whittier. Signed by: Dr. Smith Yeats. Date: March 15, 2023, 14:30\n",
        "\"\"\"\n",
        "# manually annotate entity_text\n",
        "ground_truth = pd.DataFrame(\n",
        "    {\n",
        "    \"entity_text\": [\"Emma Su\", \"Lee\", \"Rob\", \"Liv\", \"Dave\", \"Pete\", \"Jen K\", \"Kate\", \"Smith\", \"mike\", \"Emma\", \"Smith Y.\", \"Kate W.\", \"Kate Whittier\", \"Smith Yeats\"],\n",
        "    \"entity_start_index\": [9, 255, 354, 369, 378, 457, 575, 889, 923, 1021, 1261, 1346, 1365, 1388, 1418],\n",
        "    \"entity_end_index\": [16, 258, 357, 372, 382, 461, 580, 893, 928, 1025, 1265, 1354, 1372, 1401, 1429]\n",
        "})\n",
        "ground_truth.head()"
      ],
      "metadata": {
        "id": "XQhbKMbgVPCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Microsoft Presidio Model\n",
        "\n",
        "[Microsoft Presidio](https://github.com/microsoft/presidio) is an open-source library for detecting personally identifiable information (PII) in text. In this section, we apply the Presidio recognizer to our synthetic note to automatically detect names. Later, we will compare the predicted entities with our ground truth annotations.\n",
        "\n",
        "Presidio outputs start/end indices and entity types, which can be directly compared to ground truth for evaluation."
      ],
      "metadata": {
        "id": "1IstNltexOBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize presidio_analyzer\n",
        "analyzer = AnalyzerEngine()\n",
        "# detect PII\n",
        "results = analyzer.analyze(text=note_text, entities=[\"PERSON\"], language=\"en\")\n",
        "df_presidio = pd.DataFrame([\n",
        "    {\n",
        "        \"model\":\"presidio\",\n",
        "        \"entity_type\":ent.entity_type,\n",
        "        \"entity_start_index\":ent.start,\n",
        "        \"entity_end_index\":ent.end,\n",
        "        \"entity_text\":note_text[ent.start:ent.end]\n",
        "    } for ent in results\n",
        "])\n",
        "df_presidio.head()"
      ],
      "metadata": {
        "id": "V7PV0NfRWxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative NER Models from HuggingFace\n",
        "- BERT-Base NER ([dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)):\n",
        "A smaller BERT model fine-tuned on the CoNLL-2003 dataset to recognize person names and other standard entities.\n",
        "\n",
        "- BERT-Large NER ([dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)):\n",
        "A larger cased BERT model fine-tuned similarly on the CoNLL-2003 dataset, expected to provide better context understanding due to more parameters.\n",
        "\n",
        "We use the HuggingFace pipeline API for NER and extract entities including start/end positions to compare with ground truth."
      ],
      "metadata": {
        "id": "fveTVtsFyACX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a small NER model for demo\n",
        "bert_base = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "ents = bert_base(note_text)\n",
        "df_bert_base = pd.DataFrame([\n",
        "    {\n",
        "        \"model\":\"bert_base\",\n",
        "        \"entity_type\": ent[\"entity_group\"],\n",
        "        \"entity_start_index\": ent[\"start\"],\n",
        "        \"entity_end_index\": ent[\"end\"],\n",
        "        \"entity_text\": ent[\"word\"]\n",
        "    } for ent in ents\n",
        "])\n",
        "df_bert_base.head()"
      ],
      "metadata": {
        "id": "JY6E2huhWrfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Build NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "# Run NER\n",
        "ents = ner_pipeline(note_text)\n",
        "df_bert_finetuned = pd.DataFrame([\n",
        "    {\n",
        "        \"model\": \"bert_finetuned\",\n",
        "        \"entity_type\": ent[\"entity_group\"],\n",
        "        \"entity_start_index\": ent[\"start\"],\n",
        "        \"entity_end_index\": ent[\"end\"],\n",
        "        \"entity_text\": ent[\"word\"]\n",
        "    } for ent in ents\n",
        "])\n",
        "df_bert_finetuned.head()"
      ],
      "metadata": {
        "id": "Qxk0Tw56gpWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "Model performance is evaluated using overlap ratio thresholds. If a predicted span overlaps a ground truth span above a threshold, it is counted as a true positive; otherwise, it is a false negative.\n",
        "\n",
        "Metrics computed include:\n",
        "\n",
        "Precision: TP / (TP + FP) – indicates how much correct information is retained without over-censoring.\n",
        "\n",
        "Recall: TP / (TP + FN) – more critical for de-identification, since missing a PHI entity could result in sensitive information leakage.\n",
        "\n",
        "Multiple thresholds are applied to assess robustness, and Seaborn line plots are used to visualize precision and recall across thresholds for each model.\n",
        "\n",
        "Note: Because this project uses synthetic data, the exact ranking or performance differences between models may not fully reflect real-world behavior, but it effectively demonstrates the methodology and evaluation approach."
      ],
      "metadata": {
        "id": "5cdT8YURznXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ner_models(ground_truth, model_dfs, model_names, thresholds=[0.1, 0.2, 0.3, 0.4, 0.5]):\n",
        "    \"\"\"\n",
        "    Evaluate multiple NER models against ground truth using overlap-based matching.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth : pd.DataFrame\n",
        "        DataFrame containing ground truth entities with columns:\n",
        "        ['entity_text', 'entity_start_index', 'entity_end_index'].\n",
        "    model_dfs : list of pd.DataFrame\n",
        "        List of predicted entities DataFrames, each with the same columns as ground_truth.\n",
        "    model_names : list of str\n",
        "        Names of the models corresponding to model_dfs.\n",
        "    thresholds : list of float, optional\n",
        "        Minimum overlap ratio to consider a predicted entity as a true positive.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing precision and recall for each model at each threshold.\n",
        "        Columns: ['model', 'threshold', 'precision', 'recall'].\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for model_df, model_name in zip(model_dfs, model_names):\n",
        "        for t in thresholds:\n",
        "            tp, fp, fn = 0, 0, 0\n",
        "            matched_gt_idx = set()\n",
        "\n",
        "            # Iterate over predicted entities\n",
        "            for _, m_row in model_df.iterrows():\n",
        "                m_start, m_end = m_row['entity_start_index'], m_row['entity_end_index']\n",
        "                match_found = False\n",
        "\n",
        "                for gt_idx, gt_row in ground_truth.iterrows():\n",
        "                    gt_start, gt_end = gt_row['entity_start_index'], gt_row['entity_end_index']\n",
        "\n",
        "                    # Compute overlap\n",
        "                    overlap = max(0, min(m_end, gt_end) - max(m_start, gt_start))\n",
        "                    overlap_ratio = overlap / (gt_end - gt_start)\n",
        "\n",
        "                    if overlap_ratio >= t:\n",
        "                        tp += 1\n",
        "                        matched_gt_idx.add(gt_idx)\n",
        "                        match_found = True\n",
        "                        break\n",
        "\n",
        "                if not match_found:\n",
        "                    fp += 1\n",
        "\n",
        "            fn = len(ground_truth) - len(matched_gt_idx)\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "            results.append({\n",
        "                'model': model_name,\n",
        "                'threshold': t,\n",
        "                'precision': precision,\n",
        "                'recall': recall\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "N7T0YwOddFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = \\\n",
        "evaluate_ner_models(ground_truth,\n",
        "                    model_dfs=[df_presidio, df_bert_base, df_bert_finetuned],\n",
        "                    model_names=[\"Presidio\", \"BERT Base\", \"BERT Finetuned\"],\n",
        "                    thresholds = [0.1, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "evaluation_result"
      ],
      "metadata": {
        "id": "rID3em3dnCyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ner_score(results_df, metric='precision'):\n",
        "    \"\"\"\n",
        "    Plot NER evaluation score for multiple models across thresholds.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    results_df : pd.DataFrame\n",
        "        DataFrame from evaluate_ner_models containing columns:\n",
        "        ['model', 'threshold', 'precision', 'recall'].\n",
        "    metric : str, optional\n",
        "        Metric to plot. Must be either 'precision' or 'recall'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Displays a line plot for the selected metric.\n",
        "    \"\"\"\n",
        "    if metric not in ['precision', 'recall']:\n",
        "        raise ValueError(\"metric must be 'precision' or 'recall'\")\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.lineplot(data=results_df, x='threshold', y=metric, hue='model', marker='o')\n",
        "    plt.ylim(0.5, 1.1)\n",
        "    plt.title(f\"NER Model Evaluation: {metric.capitalize()} vs Threshold\")\n",
        "    plt.xlabel(\"Overlap Threshold\")\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zmXfgzjEniyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ner_score(evaluation_result, \"recall\")"
      ],
      "metadata": {
        "id": "UBAPUI_ipOIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ner_score(evaluation_result, \"precision\")"
      ],
      "metadata": {
        "id": "j3smnjFbrq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qa82PN9bveN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}